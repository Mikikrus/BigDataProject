{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7138e9c2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Project description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743277",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The project is about predicting the number of recommendations of a comment performed on New York Times Comments\n",
    "dataset that can be found under the link https://www.kaggle.com/datasets/aashita/nyt-comments.\n",
    "Main challanges of this project are:\n",
    "* handling large data volume\n",
    "* feature engineering\n",
    "* etc..\n",
    "To "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844bb535",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab31dec4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DateType\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c4e1bf4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setMaster(\"local[8]\").setAppName(\"big_data\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c177d47c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18505333",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3881ec7b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_spark_df(path):\n",
    "    return spark.read.option(\"multiline\",True).option('lineSep','\\n').option(\"header\", True).option(\"delimiter\", \",\").option(\"inferSchema\",True).csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6ef6f29",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "articles_df = read_spark_df('data/nyt-articles-2020.csv')\n",
    "comments_df = read_spark_df('data/nyt-comments-2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f2d056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = comments_df.limit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62879223",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = comments_df.withColumn(\"updateDate\", comments_df['updateDate'].cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a000ac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- newsdesk: string (nullable = true)\n",
      " |-- section: string (nullable = true)\n",
      " |-- subsection: string (nullable = true)\n",
      " |-- material: string (nullable = true)\n",
      " |-- headline: string (nullable = true)\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      " |-- word_count: string (nullable = true)\n",
      " |-- pub_date: string (nullable = true)\n",
      " |-- n_comments: string (nullable = true)\n",
      " |-- uniqueID\\r: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "articles_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af46e756",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- commentID: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- commentSequence: integer (nullable = true)\n",
      " |-- userID: integer (nullable = true)\n",
      " |-- userDisplayName: string (nullable = true)\n",
      " |-- userLocation: string (nullable = true)\n",
      " |-- userTitle: string (nullable = true)\n",
      " |-- commentBody: string (nullable = true)\n",
      " |-- createDate: string (nullable = true)\n",
      " |-- updateDate: string (nullable = true)\n",
      " |-- approveDate: string (nullable = true)\n",
      " |-- recommendations: string (nullable = true)\n",
      " |-- replyCount: string (nullable = true)\n",
      " |-- editorsSelection: string (nullable = true)\n",
      " |-- parentID: string (nullable = true)\n",
      " |-- parentUserDisplayName: string (nullable = true)\n",
      " |-- depth: string (nullable = true)\n",
      " |-- commentType: string (nullable = true)\n",
      " |-- trusted: string (nullable = true)\n",
      " |-- recommendedFlag: integer (nullable = true)\n",
      " |-- permID: string (nullable = true)\n",
      " |-- isAnonymous: string (nullable = true)\n",
      " |-- articleID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5458492c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can observe that although we used inferSchema some of the columns should be stored as a different data type. Let's fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c07c2919",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "comments_df=comments_df.withColumn('recommendations',comments_df['recommendations'].cast(\"float\"))\\\n",
    "                        .withColumn(\"createDate\", comments_df['createDate'].cast(DateType()))\\\n",
    "                        .withColumn(\"updateDate\", comments_df['updateDate'].cast(DateType()))\\\n",
    "                        .withColumn(\"approveDate\", comments_df['approveDate'].cast(DateType()))\\\n",
    "                        .withColumn('replyCount',comments_df['replyCount'].cast(\"int\"))\\\n",
    "                        .withColumn('depth',comments_df['depth'].cast(\"int\"))\\\n",
    "                        .withColumn('isAnonymous',comments_df['isAnonymous'].cast(\"int\"))\\\n",
    "                        .withColumn('editorsSelection',comments_df['editorsSelection'].cast(\"int\"))\n",
    "#actually some of the above columns are boolean but pyspark does not provide such datatype so we cast them to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdceb49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = articles_df.withColumn('word_count',articles_df['word_count'].cast(\"int\"))\\\n",
    "                         .withColumn(\"pub_date\", articles_df['pub_date'].cast(DateType()))\\\n",
    "                         .withColumn('n_comments',articles_df['n_comments'].cast(\"int\"))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0672ddd3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- commentID: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- commentSequence: integer (nullable = true)\n",
      " |-- userID: integer (nullable = true)\n",
      " |-- userDisplayName: string (nullable = true)\n",
      " |-- userLocation: string (nullable = true)\n",
      " |-- userTitle: string (nullable = true)\n",
      " |-- commentBody: string (nullable = true)\n",
      " |-- createDate: date (nullable = true)\n",
      " |-- updateDate: date (nullable = true)\n",
      " |-- approveDate: date (nullable = true)\n",
      " |-- recommendations: float (nullable = true)\n",
      " |-- replyCount: integer (nullable = true)\n",
      " |-- editorsSelection: integer (nullable = true)\n",
      " |-- parentID: string (nullable = true)\n",
      " |-- parentUserDisplayName: string (nullable = true)\n",
      " |-- depth: integer (nullable = true)\n",
      " |-- commentType: string (nullable = true)\n",
      " |-- trusted: string (nullable = true)\n",
      " |-- recommendedFlag: integer (nullable = true)\n",
      " |-- permID: string (nullable = true)\n",
      " |-- isAnonymous: integer (nullable = true)\n",
      " |-- articleID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04073825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- newsdesk: string (nullable = true)\n",
      " |-- section: string (nullable = true)\n",
      " |-- subsection: string (nullable = true)\n",
      " |-- material: string (nullable = true)\n",
      " |-- headline: string (nullable = true)\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      " |-- word_count: integer (nullable = true)\n",
      " |-- pub_date: date (nullable = true)\n",
      " |-- n_comments: integer (nullable = true)\n",
      " |-- uniqueID\\r: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "articles_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd7a6f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's take a look at our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86a97d42",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|  recommendations|\n",
      "+-------+-----------------+\n",
      "|  count|            10000|\n",
      "|   mean|          20.9887|\n",
      "| stddev|97.33120007767721|\n",
      "|    min|              0.0|\n",
      "|    max|           3816.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments_df.describe(['recommendations']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f052ebcc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can observe that data is contains outliers. We will get rid of them using quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a13186e8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "upper_limit = comments_df.approxQuantile('recommendations', [ 0.9], 0.05)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3155001e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "comments_df = comments_df.filter((col('recommendations')<upper_limit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "699dcac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|  recommendations|\n",
      "+-------+-----------------+\n",
      "|  count|             8512|\n",
      "|   mean|5.095277255639098|\n",
      "| stddev|5.478552516337315|\n",
      "|    min|              0.0|\n",
      "|    max|             23.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments_df.describe(['recommendations']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "522a13f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gre_histogram = comments_df.select('recommendations').rdd.flatMap(lambda x: x).histogram(11)\n",
    "\n",
    "# Loading the Computed Histogram into a Pandas Dataframe for plotting\n",
    "pd.DataFrame(\n",
    "    list(zip(*gre_histogram)), \n",
    "    columns=['bin', 'frequency']\n",
    ").set_index(\n",
    "    'bin'\n",
    ").plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3331b602",
   "metadata": {},
   "source": [
    "Transformr text to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ceb73322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.mllib.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd51356c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9645696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover,Word2Vec\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7763206",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df=comments_df.withColumn(\"createDateInt\", F.unix_timestamp(comments_df['createDate']))\n",
    "\n",
    "data = articles_df.select(col(\"abstract\"),col(\"pub_date\"),\n",
    "                          col(\"keywords\"),\n",
    "                          col(\"uniqueID\\r\").alias('articleID'))\n",
    "data = data.withColumn(\"articleID\",expr(\"substring(articleID, 1, length(articleID)-1)\"))\n",
    "full_df = comments_df.join(data, on=[\"articleID\"])\n",
    "full_df = full_df.withColumn(\"pubDateInt\", F.unix_timestamp(full_df['pub_date']))\n",
    "\n",
    "full_df = full_df.withColumn('time_passed', abs(full_df['pubDateInt'] - full_df['createDateInt']))\n",
    "\n",
    "# a = comments_df.select('articleID').distinct().take(1)\n",
    "# b = data.select(col('articleID')).take(1)\n",
    "# print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec44f31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- articleID: string (nullable = true)\n",
      " |-- commentID: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- commentSequence: integer (nullable = true)\n",
      " |-- userID: integer (nullable = true)\n",
      " |-- userDisplayName: string (nullable = true)\n",
      " |-- userLocation: string (nullable = true)\n",
      " |-- userTitle: string (nullable = true)\n",
      " |-- commentBody: string (nullable = true)\n",
      " |-- createDate: date (nullable = true)\n",
      " |-- updateDate: date (nullable = true)\n",
      " |-- approveDate: date (nullable = true)\n",
      " |-- recommendations: float (nullable = true)\n",
      " |-- replyCount: integer (nullable = true)\n",
      " |-- editorsSelection: integer (nullable = true)\n",
      " |-- parentID: string (nullable = true)\n",
      " |-- parentUserDisplayName: string (nullable = true)\n",
      " |-- depth: integer (nullable = true)\n",
      " |-- commentType: string (nullable = true)\n",
      " |-- trusted: string (nullable = true)\n",
      " |-- recommendedFlag: integer (nullable = true)\n",
      " |-- permID: string (nullable = true)\n",
      " |-- isAnonymous: integer (nullable = true)\n",
      " |-- createDateInt: long (nullable = true)\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- pub_date: date (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      " |-- pubDateInt: long (nullable = true)\n",
      " |-- time_passed: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d70fe22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(df,column='commentBody'):\n",
    "    df = df.withColumn(column, trim(regexp_replace(column,'(@\\w+)|[^a-zA-Z\\s]', '')))\n",
    "    df = df.select(split(col(column),\" \").alias(column))\n",
    "    remover = StopWordsRemover(inputCol=column, outputCol=\"filtered\")\n",
    "    filtered = remover.transform(df)\n",
    "    word2vec = Word2Vec(inputCol=\"filtered\", outputCol=\"vector\")\n",
    "    model = word2vec.fit(filtered)\n",
    "    return model.transform(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0e878216",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_vector(full_df)\n",
    "df2 = get_vector(full_df, column='abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6fb141d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|         commentBody|            filtered|              vector|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[Nothing, here, i...|[Nothing, truthfu...|[0.05439707271399...|\n",
      "|[The, legal, thri...|[legal, thriller,...|[0.02881309679812...|\n",
      "|[This, is, a, gre...|[great, case, stu...|[0.04588875124696...|\n",
      "|[Add, forprofit, ...|[Add, forprofit, ...|[-0.0375565004069...|\n",
      "|[S, I, think, the...|[think, point, hu...|[0.01957566989585...|\n",
      "|[S, Thats, agreed...|[Thats, agreed, J...|[0.07047333251684...|\n",
      "|[Firstly, everyon...|[Firstly, everyon...|[-0.0113774585701...|\n",
      "|[Protect, veteran...|[Protect, veteran...|[0.06493437384842...|\n",
      "|[Massive, subsidi...|[Massive, subsidi...|[-0.0162929578117...|\n",
      "|[This, veteran, w...|[veteran, widowed...|[-0.0080836191225...|\n",
      "|[Since, the, pers...|[Since, person, s...|[-0.0032923898543...|\n",
      "|[Sorry, about, yo...|[Sorry, treatment...|[0.09255785489920...|\n",
      "|[I, am, a, vetera...|[veteran, say, co...|[-0.0070048579638...|\n",
      "|[In, America, the...|[America, thin, l...|[-0.0020349243121...|\n",
      "|[Kakistocracy, at...|[Kakistocracy, fi...|[0.00443085562437...|\n",
      "|[protect, not, ju...| [protect, veterans]|[-0.0889553595334...|\n",
      "|[The, Trump, admi...|[Trump, administr...|[-0.0031893252429...|\n",
      "|[I, am, betting, ...|[betting, Betsy, ...|[-0.0148533173130...|\n",
      "|[Was, Trump, U, p...|[Trump, U, part, ...|[0.10759306326508...|\n",
      "|[THANK, YOUI, jus...|[THANK, YOUI, wis...|[-0.0172781767880...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|            abstract|            filtered|              vector|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[Congress, could,...|[Congress, much, ...|[0.12168299489551...|\n",
      "|[Congress, could,...|[Congress, much, ...|[0.12168299489551...|\n",
      "|[Congress, could,...|[Congress, much, ...|[0.12168299489551...|\n",
      "|[Congress, could,...|[Congress, much, ...|[0.12168299489551...|\n",
      "|[Congress, could,...|[Congress, much, ...|[0.12168299489551...|\n",
      "|[Congress, could,...|[Congress, much, ...|[0.12168299489551...|\n",
      "|[Congress, could,...|[Congress, much, ...|[0.12168299489551...|\n",
      "|[Congress, could,...|[Congress, much, ...|[0.12168299489551...|\n",
      "|[Congress, could,...|[Congress, much, ...|[0.12168299489551...|\n",
      "|[Congress, could,...|[Congress, much, ...|[0.12168299489551...|\n",
      "|[Congress, could,...|[Congress, much, ...|[0.12168299489551...|\n",
      "|[Congress, could,...|[Congress, much, ...|[0.12168299489551...|\n",
      "|[Congress, could,...|[Congress, much, ...|[0.12168299489551...|\n",
      "|[Congress, could,...|[Congress, much, ...|[0.12168299489551...|\n",
      "|[Congress, could,...|[Congress, much, ...|[0.12168299489551...|\n",
      "|[Congress, could,...|[Congress, much, ...|[0.12168299489551...|\n",
      "|[Congress, could,...|[Congress, much, ...|[0.12168299489551...|\n",
      "|[Congress, could,...|[Congress, much, ...|[0.12168299489551...|\n",
      "|[Congress, could,...|[Congress, much, ...|[0.12168299489551...|\n",
      "|[Congress, could,...|[Congress, much, ...|[0.12168299489551...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3e30dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments_df=comments_df.withColumn(\"createDateInt\",comments_df.createDateInt.cast('double'))\n",
    "# # df=df.withColumn(\"vector\",df.vector.cast('array<bigint>'))\n",
    "# df.withColumn(\n",
    "#     \"vector\", \n",
    "#     F.array_union(df.vector, F.array(comments_df.createDateInt))\n",
    "# ).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "546e93d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "full_df = full_df.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "full_df = full_df.join(df, on=[\"row_index\"]).drop(\"row_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "01a016b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.select(col(\"vector\").alias('abstract_vector'))\n",
    "df2 = df2.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "full_df = full_df.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "full_df = full_df.join(df2, on=[\"row_index\"]).drop(\"row_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eebcf3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "numericCols = ['vector', 'abstract_vector', 'time_passed']\n",
    "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
    "full_df = assembler.transform(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "194ce676",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.drop(*numericCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "facc5081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "bucketizer = Bucketizer(splits=[ 0, 6, 12, 18, float('Inf') ],inputCol=\"recommendations\", outputCol=\"buckets\")\n",
    "full_df = bucketizer.setHandleInvalid(\"keep\").transform(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "05e2ac1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- articleID: string (nullable = true)\n",
      " |-- commentID: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- commentSequence: integer (nullable = true)\n",
      " |-- userID: integer (nullable = true)\n",
      " |-- userDisplayName: string (nullable = true)\n",
      " |-- userLocation: string (nullable = true)\n",
      " |-- userTitle: string (nullable = true)\n",
      " |-- commentBody: string (nullable = true)\n",
      " |-- createDate: date (nullable = true)\n",
      " |-- updateDate: date (nullable = true)\n",
      " |-- approveDate: date (nullable = true)\n",
      " |-- recommendations: float (nullable = true)\n",
      " |-- replyCount: integer (nullable = true)\n",
      " |-- editorsSelection: integer (nullable = true)\n",
      " |-- parentID: string (nullable = true)\n",
      " |-- parentUserDisplayName: string (nullable = true)\n",
      " |-- depth: integer (nullable = true)\n",
      " |-- commentType: string (nullable = true)\n",
      " |-- trusted: string (nullable = true)\n",
      " |-- recommendedFlag: integer (nullable = true)\n",
      " |-- permID: string (nullable = true)\n",
      " |-- isAnonymous: integer (nullable = true)\n",
      " |-- createDateInt: long (nullable = true)\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- pub_date: date (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      " |-- pubDateInt: long (nullable = true)\n",
      " |-- commentBody: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- buckets: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "96364aff",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: DataFrame must have a single vector type column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-cec7a8476480>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRowMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Compute the top 4 principal components.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Principal components are stored in a local dense matrix.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/mllib/linalg/distributed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rows, numRows, numCols)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mjava_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"createRowMatrix\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumRows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumCols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mjava_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"createRowMatrix\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumRows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumCols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         elif (isinstance(rows, JavaObject)\n\u001b[1;32m    107\u001b[0m               and rows.getClass().getSimpleName() == \"RowMatrix\"):\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: DataFrame must have a single vector type column"
     ]
    }
   ],
   "source": [
    "#### TODO ####\n",
    "\n",
    "# from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "# from pyspark.mllib.linalg import Vectors, VectorUDT\n",
    "# from pyspark.sql.functions import udf, array\n",
    "\n",
    "# mat = RowMatrix(full_df.select('features'))\n",
    "# pc = mat.computePrincipalComponents(10)\n",
    "# projected = mat.multiply(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c0f83cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+--------------------+\n",
      "|predictedLabel|buckets|            features|\n",
      "+--------------+-------+--------------------+\n",
      "|           0.0|    3.0|[-0.0213082146025...|\n",
      "|           0.0|    2.0|[0.00879926614080...|\n",
      "|           0.0|    0.0|[-0.0183993745595...|\n",
      "|           0.0|    0.0|(201,[100,101,102...|\n",
      "|           0.0|    0.0|[0.01398731380322...|\n",
      "|           0.0|    0.0|[0.03632894159718...|\n",
      "|           0.0|    0.0|[0.03271815881181...|\n",
      "|           0.0|    0.0|[0.05916589977568...|\n",
      "|           0.0|    0.0|[0.07192069188588...|\n",
      "|           0.0|    0.0|[0.08958277180499...|\n",
      "|           0.0|    0.0|[0.02133534631866...|\n",
      "|           0.0|    0.0|[0.02119789324014...|\n",
      "|           0.0|    0.0|[-0.0079683041221...|\n",
      "|           0.0|    0.0|[0.00568002799179...|\n",
      "|           0.0|    0.0|[0.01157510414682...|\n",
      "+--------------+-------+--------------------+\n",
      "only showing top 15 rows\n",
      "\n",
      "Test Error = 0.335054\n",
      "RandomForestClassificationModel: uid=RandomForestClassifier_3e3fb794058f, numTrees=20, numClasses=4, numFeatures=201\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = full_df\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"buckets\", outputCol=\"indexedLabel\").fit(data)\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"buckets\", featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"buckets\", \"features\").show(15)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"buckets\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel = model.stages[2]\n",
    "print(rfModel)  # summary only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
